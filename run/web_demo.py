import argparse
import random

import torch
import gradio as gr

from call import load_model, generate_response


def predict(user_input: str,
            chat_history: list,
            model, proc, device):
    # chat_history + æ–°è¾“å…¥
    past = []
    for q, r in chat_history:
        past.extend([q, r])
    past.append(user_input)
    
    if user_input[-1] not in [',', 'ã€‚', 'ï¼', 'ï¼Ÿ']:
        user_input += random.choice(['ã€‚', 'ï¼', 'ï¼Ÿ', ''])
    
    response = generate_response(model, proc, past, device)
    chat_history = chat_history + [(user_input, response)]
    
    return chat_history, chat_history

def regenerate(chat_history: list,
               model, proc, device):
    if not chat_history:
        return [], []
    last_q, _ = chat_history[-1]
    chat_history = chat_history[:-1]
    
    past = []
    for q, r in chat_history:
        past.extend([q, r])
    past.append(last_q)
    
    response = generate_response(model, proc, past, device)
    chat_history = chat_history + [(last_q, response)]
    
    return chat_history, chat_history

def clear_history():
    return [], []


def deploy(model_path: str, tokenizer_path: str, device):
    model, proc = load_model(model_path, tokenizer_path, device)
    
    with gr.Blocks(title="Gradio") as demo:
        # é¡¶éƒ¨æ ‡é¢˜ & æè¿°
        gr.Markdown("""<center><font size=8>ChatBot</center>""")
        gr.Markdown(
            """<center><font size=3>This WebUI is based on GPT-style ChatBot, developed by Z.Y. Lin.</center>""")

        # Chatbot ç»„ä»¶ & çŠ¶æ€ & æ–‡æœ¬è¾“å…¥
        chatbot = gr.Chatbot(label="ChatBot", height=400)
        state = gr.State([])  # ç”¨æ¥å­˜æ”¾ list of (q, r)
        txt = gr.Textbox(lines=2,
                         placeholder="è¯·è¾“å…¥ä¸­æ–‡...",
                         label="Input")

        # æ“ä½œæŒ‰é’®
        with gr.Row():
            btn_clear = gr.Button("ğŸ§¹ Clear History (æ¸…é™¤å†å²)")
            btn_submit = gr.Button("ğŸš€ Submit (å‘é€)")
            btn_regen = gr.Button("ğŸ¤”ï¸ Regenerate (é‡è¯•)")

        # Submitï¼šå‘é€æ–°æ¶ˆæ¯
        btn_submit.click(
            fn=lambda user_input, history: predict(user_input, history, model, proc, device),
            inputs=[txt, state],
            outputs=[chatbot, state]
        )
        
        # å›è½¦ä¹Ÿå‘æ¶ˆæ¯
        txt.submit(
            fn=lambda user_input, history: predict(user_input, history, model, proc, device),
            inputs=[txt, state],
            outputs=[chatbot, state]
        )
        
        # æ¸…ç©ºè¾“å…¥æ¡†
        btn_submit.click(lambda _: "", inputs=txt, outputs=txt)
        txt.submit(lambda _: "", inputs=txt, outputs=txt)

        # Regenerateï¼šé‡æ–°ç”Ÿæˆæœ€åä¸€æ¡å›å¤
        btn_regen.click(
            fn=lambda history: regenerate(history, model, proc, device),
            inputs=[state],
            outputs=[chatbot, state]
        )

        # Clearï¼šæ¸…ç©ºå†å²
        btn_clear.click(
            fn=clear_history,
            inputs=None,
            outputs=[chatbot, state]
        )

        # åº•éƒ¨ç‰ˆæƒ
        gr.HTML("""
        <div style="text-align:center; margin-top:1em;">
          <small>Â© 2025 Z.Y. Lin</small>
        </div>
        """)
    
    demo.launch(share=False)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--model', type=str, default='chatbot.pt')
    parser.add_argument('--tokenizer', type=str, default='tokenizer.json')
    args = parser.parse_args()
    
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    deploy(args.model, args.tokenizer, device)