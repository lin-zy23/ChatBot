import argparse
import random

import torch
import gradio as gr

from utils import load_model, generate_response


PUNCTS = ['ã€‚', 'ï¼', 'ï¼Ÿ']

def predict(user_input: str,
            chat_history: list,
            model, proc, device):
    original = user_input
    
    prompt_input = original
    # if prompt_input and prompt_input[-1] not in [',','ã€‚','ï¼','ï¼Ÿ','~']:
    #     prompt_input += random.choice(PUNCTS)
    
    past = [msg["content"] for msg in chat_history] + [prompt_input]
    response = generate_response(model, proc, past, device)
    
    chat_history = chat_history + [
        {"role": "user", "content": original},
        {"role": "assistant", "content": response}
    ]

    return chat_history, chat_history

def regenerate(chat_history: list, model, proc, device):
    if len(chat_history) < 2:
        return [], []
    
    last_assistant = chat_history.pop()
    last_user      = chat_history.pop()
    
    original = last_user["content"]
    
    prompt_input = original
    if prompt_input and prompt_input[-1] not in [',','ã€‚','ï¼','ï¼Ÿ','~']:
        prompt_input += random.choice(PUNCTS)
    
    past = [msg["content"] for msg in chat_history] + [prompt_input]
    new_resp = generate_response(model, proc, past, device)
    
    chat_history += [
        last_user,
        {"role": "assistant", "content": new_resp}
    ]

    return chat_history, chat_history


def clear_history():
    return [], []


def deploy(model_path: str, tokenizer_path: str, device):
    model, proc = load_model(model_path, tokenizer_path, device)
    
    with gr.Blocks(title="Gradio") as demo:
        # é¡¶éƒ¨æ ‡é¢˜ & æè¿°
        gr.Markdown("""<center><font size=8>ChatBot</center>""")
        gr.Markdown(
            """<center><font size=3>This WebUI is based on GPT-style ChatBot, developed by Z.Y. Lin.</center>""")

        # Chatbot ç»„ä»¶ & çŠ¶æ€ & æ–‡æœ¬è¾“å…¥
        chatbot = gr.Chatbot(label="ChatBot", height=400, type="messages")
        state = gr.State([])  # ç”¨æ¥å­˜æ”¾ list of (q, r)
        txt = gr.Textbox(lines=2,
                         placeholder="è¯·è¾“å…¥ä¸­æ–‡...",
                         label="Input")

        # æ“ä½œæŒ‰é’®
        with gr.Row():
            btn_clear = gr.Button("ğŸ§¹ Clear History (æ¸…é™¤å†å²)")
            btn_submit = gr.Button("ğŸš€ Submit (å‘é€)")
            btn_regen = gr.Button("ğŸ¤”ï¸ Regenerate (é‡è¯•)")

        # Submitï¼šå‘é€æ–°æ¶ˆæ¯
        btn_submit.click(
            fn=lambda user_input, history: predict(user_input, history, model, proc, device),
            inputs=[txt, state],
            outputs=[chatbot, state]
        )
        
        # å›è½¦ä¹Ÿå‘æ¶ˆæ¯
        txt.submit(
            fn=lambda user_input, history: predict(user_input, history, model, proc, device),
            inputs=[txt, state],
            outputs=[chatbot, state]
        )
        
        # æ¸…ç©ºè¾“å…¥æ¡†
        btn_submit.click(lambda _: "", inputs=txt, outputs=txt)
        txt.submit(lambda _: "", inputs=txt, outputs=txt)

        # Regenerateï¼šé‡æ–°ç”Ÿæˆæœ€åä¸€æ¡å›å¤
        btn_regen.click(
            fn=lambda history: regenerate(history, model, proc, device),
            inputs=[state],
            outputs=[chatbot, state]
        )

        # Clearï¼šæ¸…ç©ºå†å²
        btn_clear.click(
            fn=clear_history,
            inputs=None,
            outputs=[chatbot, state]
        )

        # åº•éƒ¨ç‰ˆæƒ
        gr.HTML("""
        <div style="text-align:center; margin-top:1em;">
          <small>Â© 2025 Z.Y. Lin</small>
        </div>
        """)
    
    demo.launch(share=False)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--model', type=str, default='chatbot.pt')
    parser.add_argument('--tokenizer', type=str, default='tokenizer.json')
    args = parser.parse_args()
    
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    deploy(args.model, args.tokenizer, device)
